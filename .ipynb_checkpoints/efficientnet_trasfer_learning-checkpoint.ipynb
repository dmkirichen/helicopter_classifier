{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import sys\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.compat.v1 import disable_eager_execution\n",
    "from keras.backend import clear_session\n",
    "\n",
    "disable_eager_execution()\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "\n",
    "width = 224\n",
    "height = 224\n",
    "epochs = 20\n",
    "NUM_TRAIN = 240\n",
    "NUM_TEST = 60\n",
    "dropout_rate = 0.2\n",
    "input_shape = (height, width, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
    "# Higher the number, the more complex the model is.\n",
    "from efficientnet_keras_transfer_learning.efficientnet import EfficientNetB0 as Net\n",
    "from efficientnet_keras_transfer_learning.efficientnet import center_crop_and_resize, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dmkirichen/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# loading pretrained conv base model\n",
    "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ansat  ka-226t\tka-27  ka-31  ka-32a11bc  ka-52  mi-24\tmi-26t\tmi-28n\tmi-8\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes for every class: (30, 30, 30, 30, 30, 30, 30, 30, 30, 30)\n"
     ]
    }
   ],
   "source": [
    "# The path to the directory where the original\n",
    "# dataset was uncompressed\n",
    "original_dataset_dir = './data/'\n",
    "\n",
    "ansat_images = glob.glob(os.path.join(original_dataset_dir, 'ansat', '*.jpg'))\n",
    "ka226t_images = glob.glob(os.path.join(original_dataset_dir, 'ka-226t', '*.jpg'))\n",
    "ka27_images = glob.glob(os.path.join(original_dataset_dir, 'ka-27', '*.jpg'))\n",
    "ka31_images = glob.glob(os.path.join(original_dataset_dir, 'ka-31', '*.jpg'))\n",
    "ka32a11bc_images = glob.glob(os.path.join(original_dataset_dir, 'ka-32a11bc', '*.jpg'))\n",
    "ka52_images = glob.glob(os.path.join(original_dataset_dir, 'ka-52', '*.jpg'))\n",
    "mi24_images = glob.glob(os.path.join(original_dataset_dir, 'mi-24', '*.jpg'))\n",
    "mi26t_images = glob.glob(os.path.join(original_dataset_dir, 'mi-26t', '*.jpg'))\n",
    "mi28n_images = glob.glob(os.path.join(original_dataset_dir, 'mi-28n', '*.jpg'))\n",
    "mi8_images = glob.glob(os.path.join(original_dataset_dir, 'mi-8', '*.jpg'))\n",
    "all_images = [ansat_images, ka226t_images, ka27_images, ka31_images, ka32a11bc_images, ka52_images,\n",
    "              mi24_images, mi26t_images, mi28n_images, mi8_images]\n",
    "all_names = ['ansat', 'ka-226t', 'ka-27', 'ka-31', 'ka-32a11bc', 'ka-52', 'mi-24', 'mi-26t', 'mi-28n', 'mi-8']\n",
    "print(\"shapes for every class: {}\".format((len(ansat_images), len(ka226t_images), len(ka27_images), len(ka31_images), \n",
    "                                           len(ka32a11bc_images), len(ka52_images), len(mi24_images),\n",
    "                                           len(mi26t_images), len(mi28n_images), len(mi8_images))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = './train'\n",
    "test_dir = './test'\n",
    "\n",
    "if not os.path.isdir(train_dir) or not os.path.isdir(test_dir):\n",
    "\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    for name, images in zip(all_names, all_images):\n",
    "        os.makedirs(os.path.join(train_dir, name))\n",
    "        os.makedirs(os.path.join(test_dir, name))\n",
    "    \n",
    "        train, test = train_test_split(images, train_size=0.8, random_state=42)\n",
    "    \n",
    "        for fname in train:\n",
    "            dst = os.path.join(os.path.join(train_dir, name), os.path.basename(fname))\n",
    "            shutil.copyfile(fname, dst)\n",
    "        \n",
    "        for fname in test:\n",
    "            dst = os.path.join(os.path.join(test_dir, name), os.path.basename(fname))\n",
    "            shutil.copyfile(fname, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 240 images belonging to 10 classes.\n",
      "Found 60 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.compat.v1.image import rgb_to_grayscale\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')#, preprocessing_function=rgb_to_grayscale)\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)#, preprocessing_function=rgb_to_grayscale)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to target height and width.\n",
    "        target_size=(height, width),\n",
    "        batch_size=batch_size,\n",
    "        # Since we use categorical_crossentropy loss, we need categorical labels\n",
    "        class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(height, width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
    "# model.add(layers.Flatten(name=\"flatten\"))\n",
    "if dropout_rate > 0:\n",
    "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
    "model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
    "model.add(layers.Dense(10, activation='softmax', name=\"fc_out\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnet-b0 (Model)      (None, 7, 7, 1280)        4049564   \n",
      "_________________________________________________________________\n",
      "gap (GlobalMaxPooling2D)     (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dropout_out (Dropout)        (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 256)               327936    \n",
      "_________________________________________________________________\n",
      "fc_out (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 4,380,070\n",
      "Trainable params: 4,338,054\n",
      "Non-trainable params: 42,016\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the number of trainable layers before freezing the conv base: 215\n",
      "This is the number of trainable layers after freezing the conv base: 4\n"
     ]
    }
   ],
   "source": [
    "print('This is the number of trainable layers '\n",
    "      'before freezing the conv base:', len(model.trainable_weights))\n",
    "\n",
    "conv_base.trainable = False\n",
    "\n",
    "print('This is the number of trainable layers '\n",
    "      'after freezing the conv base:', len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "240/240 [==============================] - 1047s 4s/step - loss: 4.6832 - acc: 0.1401 - val_loss: 3.5266 - val_acc: 0.1667\n",
      "Epoch 2/20\n",
      "240/240 [==============================] - 918s 4s/step - loss: 3.3992 - acc: 0.2135 - val_loss: 2.9934 - val_acc: 0.2333\n",
      "Epoch 3/20\n",
      "240/240 [==============================] - 996s 4s/step - loss: 2.6180 - acc: 0.2986 - val_loss: 2.7541 - val_acc: 0.2167\n",
      "Epoch 4/20\n",
      "240/240 [==============================] - 929s 4s/step - loss: 2.2419 - acc: 0.3576 - val_loss: 2.6984 - val_acc: 0.2000\n",
      "Epoch 5/20\n",
      "240/240 [==============================] - 1002s 4s/step - loss: 1.9558 - acc: 0.4069 - val_loss: 2.5286 - val_acc: 0.2167\n",
      "Epoch 6/20\n",
      "240/240 [==============================] - 1079s 4s/step - loss: 1.7898 - acc: 0.4521 - val_loss: 2.3321 - val_acc: 0.2833\n",
      "Epoch 7/20\n",
      "240/240 [==============================] - 959s 4s/step - loss: 1.6029 - acc: 0.4918 - val_loss: 2.2924 - val_acc: 0.3000\n",
      "Epoch 8/20\n",
      "240/240 [==============================] - 901s 4s/step - loss: 1.5245 - acc: 0.5130 - val_loss: 2.3121 - val_acc: 0.3333\n",
      "Epoch 9/20\n",
      "240/240 [==============================] - 870s 4s/step - loss: 1.4183 - acc: 0.5424 - val_loss: 2.2825 - val_acc: 0.3167\n",
      "Epoch 10/20\n",
      "240/240 [==============================] - 889s 4s/step - loss: 1.3410 - acc: 0.5627 - val_loss: 2.2038 - val_acc: 0.3500\n",
      "Epoch 11/20\n",
      "240/240 [==============================] - 859s 4s/step - loss: 1.2740 - acc: 0.5816 - val_loss: 2.2448 - val_acc: 0.2833\n",
      "Epoch 12/20\n",
      "240/240 [==============================] - 839s 3s/step - loss: 1.1493 - acc: 0.6198 - val_loss: 2.0679 - val_acc: 0.3667\n",
      "Epoch 13/20\n",
      "240/240 [==============================] - 855s 4s/step - loss: 1.1022 - acc: 0.6385 - val_loss: 2.1741 - val_acc: 0.3333\n",
      "Epoch 14/20\n",
      "240/240 [==============================] - 820s 3s/step - loss: 1.0894 - acc: 0.6465 - val_loss: 2.0072 - val_acc: 0.3667\n",
      "Epoch 15/20\n",
      "240/240 [==============================] - 831s 3s/step - loss: 1.0414 - acc: 0.6630 - val_loss: 2.0173 - val_acc: 0.3667\n",
      "Epoch 16/20\n",
      "240/240 [==============================] - 824s 3s/step - loss: 0.9710 - acc: 0.6819 - val_loss: 1.9950 - val_acc: 0.3667\n",
      "Epoch 17/20\n",
      "240/240 [==============================] - 829s 3s/step - loss: 0.9183 - acc: 0.6988 - val_loss: 2.0712 - val_acc: 0.3667\n",
      "Epoch 18/20\n",
      "240/240 [==============================] - 854s 4s/step - loss: 0.9044 - acc: 0.7014 - val_loss: 2.0572 - val_acc: 0.3833\n",
      "Epoch 19/20\n",
      "240/240 [==============================] - 886s 4s/step - loss: 0.8527 - acc: 0.7217 - val_loss: 2.0596 - val_acc: 0.3833\n",
      "Epoch 20/20\n",
      "240/240 [==============================] - 829s 3s/step - loss: 0.8351 - acc: 0.7250 - val_loss: 2.1210 - val_acc: 0.3667\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch= NUM_TRAIN,# //batch_size,\n",
    "      epochs=epochs,\n",
    "      validation_data=test_generator,\n",
    "      validation_steps= NUM_TEST,# //batch_size,\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model-14-03.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
